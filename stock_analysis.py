import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models
import sqlite3
from datetime import datetime

# Supprimer les warnings
import warnings
warnings.filterwarnings("ignore")

# UNIVERS D'INVESTISSEMENT : 22 Actions Am√©ricaines (S&P 500)
# 
# JUSTIFICATION DU CHOIX :
# =========================
# 1. DIVERSIFICATION SECTORIELLE √âQUILIBR√âE
#    - 11 secteurs diff√©rents repr√©sent√©s (GICS)
#    - 2 actions par secteur pour √©quilibre et comparaison
#    - R√©duction du risque sp√©cifique par diversification
#    - Repr√©sentation fid√®le du march√© am√©ricain
#
# 2. LIQUIDIT√â ET CAPITALISATION
#    - Toutes les actions sont des grandes capitalisations (large-cap)
#    - Volume de trading √©lev√© = ex√©cution facile
#    - Donn√©es historiques compl√®tes et fiables
#
# 3. REPR√âSENTATIVIT√â DU MARCH√â
#    - Actions issues du S&P 500 (indice de r√©f√©rence)
#    - Poids significatifs dans l'√©conomie am√©ricaine
#    - S√©lection des leaders de chaque secteur
#
# 4. ROBUSTESSE STATISTIQUE
#    - 22 actions = taille d'√©chantillon suffisante pour analyses statistiques
#    - Permet l'analyse de corr√©lations inter-secteurs
#    - Validation crois√©e sur plusieurs actifs
#    - Comparaison √©quitable entre secteurs (2 actions chacun)

STOCKS = [
    # TECHNOLOGIE (2 actions) - Secteur dominant du S&P 500
    {"symbol": "AAPL", "name": "Apple Inc."},
    {"symbol": "MSFT", "name": "Microsoft Corporation"},
    
    # FINANCE (2 actions) - Secteur cyclique important
    {"symbol": "JPM", "name": "JPMorgan Chase & Co."},
    {"symbol": "V", "name": "Visa Inc."},
    
    # SANT√â (2 actions) - Secteur d√©fensif
    {"symbol": "JNJ", "name": "Johnson & Johnson"},
    {"symbol": "UNH", "name": "UnitedHealth Group Inc."},
    
    # CONSOMMATION DISCR√âTIONNAIRE (2 actions)
    {"symbol": "TSLA", "name": "Tesla Inc."},
    {"symbol": "HD", "name": "Home Depot Inc."},
    
    # CONSOMMATION STAPLES (2 actions) - D√©fensif
    {"symbol": "WMT", "name": "Walmart Inc."},
    {"symbol": "PG", "name": "Procter & Gamble Co."},
    
    # √âNERGIE (2 actions) - Secteur cyclique
    {"symbol": "XOM", "name": "Exxon Mobil Corporation"},
    {"symbol": "CVX", "name": "Chevron Corporation"},
    
    # INDUSTRIEL (2 actions)
    {"symbol": "BA", "name": "Boeing Company"},
    {"symbol": "CAT", "name": "Caterpillar Inc."},
    
    # T√âL√âCOMMUNICATIONS (2 actions)
    {"symbol": "T", "name": "AT&T Inc."},
    {"symbol": "VZ", "name": "Verizon Communications Inc."},
    
    # MAT√âRIAUX (2 actions)
    {"symbol": "LIN", "name": "Linde plc"},
    {"symbol": "APD", "name": "Air Products and Chemicals Inc."},
    
    # UTILITAIRES (2 actions) - D√©fensif
    {"symbol": "NEE", "name": "NextEra Energy Inc."},
    {"symbol": "DUK", "name": "Duke Energy Corporation"},
    
    # IMMOBILIER (2 actions)
    {"symbol": "AMT", "name": "American Tower Corporation"},
    {"symbol": "PLD", "name": "Prologis Inc."}
]

def create_database():
    
    conn = sqlite3.connect('stock_analysis.db')
    cursor = conn.cursor()
    
    # Table avec tous les r√©sultats
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS stock_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            symbol TEXT NOT NULL,
            name TEXT NOT NULL,
            accuracy REAL NOT NULL,
            strategy_return REAL NOT NULL,
            buy_hold_return REAL NOT NULL,
            performance REAL NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    return conn

def download_stock_data(symbol):
    try:
        print(f"T√©l√©chargement de {symbol}...")
        data = yf.download(symbol, start="2020-01-01", end="2023-01-01", progress=False)
        
        if data.empty:
            raise ValueError("Donn√©es vides")
            
        print(f"‚úì {symbol}: {len(data)} jours de donn√©es")
        return data
        
    except Exception as e:
        print(f"‚úó Erreur pour {symbol}: {e}")
        # Donn√©es simul√©es simples
        dates = pd.date_range(start="2020-01-01", end="2023-01-01", freq="D")
        prices = 100 * np.cumprod(1 + np.random.normal(0.001, 0.02, len(dates)))
        
        data = pd.DataFrame({
            'Close': prices
        }, index=dates)
        
        print(f"‚úì {symbol}: donn√©es simul√©es g√©n√©r√©es")
        return data

def analyze_stock(data, symbol):
    """
    Analyser une action avec un mod√®le LSTM (Long Short-Term Memory)

    - LSTM capture les d√©pendances temporelles longues et courtes
    - Architecture classique : LSTM ‚Üí Dropout ‚Üí Dense ‚Üí Sortie sigmo√Øde
    - Fen√™tre temporelle de 20 jours pour capturer les tendances et pr√©dire le rendement N
    
    R√©f√©rences acad√©miques typiques :
    - Hochreiter & Schmidhuber (1997) : LSTM pour s√©quences temporelles
    """
    print(f"Analyse de {symbol}...")
    
    # Calculer les rendements journaliers
    data["Return"] = data["Close"].pct_change()
    data.dropna(inplace=True)
    
    # Fen√™tre temporelle : 20 jours pour capturer les tendances
    # (plus longue que le MLP pr√©c√©dent o√π c'√©tait 5 jours)
    
    
    lookback_window = 20 #60, 40 et 20 √† tester
    
    # Cr√©er les s√©quences temporelles pour le LSTM
    X, y = [], []
    for i in range(lookback_window, len(data)):
        # S√©quence de 20 rendements cons√©cutifs
        X.append(data["Return"].values[i-lookback_window:i])
        # Cible : direction du rendement du jour suivant (1 = hausse, 0 = baisse)
        y.append(1 if data["Return"].values[i] > 0 else 0)
    
    X = np.array(X)
    y = np.array(y)
    
    # Normaliser les donn√©es (important pour LSTM)
    scaler = StandardScaler()
    # Reshape pour StandardScaler : (samples, features) -> (samples*features, 1)
    X_reshaped = X.reshape(-1, 1)
    X_scaled = scaler.fit_transform(X_reshaped)
    X_scaled = X_scaled.reshape(X.shape)
    
    # Diviser en train/test (split temporel, pas al√©atoire)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # Reshape pour LSTM : (samples, timesteps, features)
    # Ici : (samples, 20, 1) - 20 pas de temps, 1 feature par pas
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
    
    #LSTM ‚Üí Dropout (r√©gularisation) ‚Üí Dense ‚Üí Sortie
    model = models.Sequential([
        # Couche LSTM avec 50 unit√©s (m√©moire)
        # return_sequences=False : on ne retourne que la derni√®re sortie
        layers.LSTM(50, activation='tanh', input_shape=(lookback_window, 1)),
        
        # Dropout pour √©viter le surapprentissage (r√©gularisation)
        layers.Dropout(0.2),

        # Couche dense pour la classification finale
        layers.Dense(25, activation='relu'),
        
        # Dropout suppl√©mentaire
        layers.Dropout(0.2),
        
        # Sortie : probabilit√© de hausse (sigmoid pour classification binaire)
        layers.Dense(1, activation='sigmoid')
    ])
    
    # Compilation avec optimiseur Adam (standard en deep learning)
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',  # Perte adapt√©e √† la classification binaire
        metrics=['accuracy']
    )
    
    # Entra√Ænement avec validation
    # epochs=5 : entra√Ænement rapide pour analyse efficace
    model.fit(
        X_train, y_train,
        epochs=5,
        batch_size=32,
        validation_split=0.1,  # 10% du train pour validation
        verbose=1
    )
    
    # Pr√©dictions sur le set de test
    predictions = model.predict(X_test, verbose=0).flatten()
    positions = (predictions > 0.5).astype(int)  # Seuil de d√©cision : 0.5
    
    # Calculer les performances de la strat√©gie
    # Les pr√©dictions correspondent aux rendements √† partir de split_idx+lookback_window
    # car chaque s√©quence X[i] pr√©dit le rendement du jour i+lookback_window
    test_start_idx = split_idx + lookback_window
    returns = data["Return"].iloc[test_start_idx:test_start_idx+len(positions)].values
    
    strategy_returns = positions * returns  # Investi seulement si pr√©diction hausse
    
    # Rendements cumul√©s
    strategy_total = np.cumprod(1 + strategy_returns)[-1] - 1
    buy_hold_total = np.cumprod(1 + returns)[-1] - 1
    performance = strategy_total - buy_hold_total
    
    # Pr√©cision : pourcentage de pr√©dictions correctes
    accuracy = np.mean(positions == y_test)
    
    print(f"‚úì {symbol}: Pr√©cision={accuracy:.2%}, Performance={performance:.2%}")
    
    return {
        'symbol': symbol,
        'accuracy': accuracy,
        'strategy_return': strategy_total,
        'buy_hold_return': buy_hold_total,
        'performance': performance
    }

def save_to_database(conn, stock_info, results):
    cursor = conn.cursor()
    cursor.execute('''
        INSERT INTO stock_results (symbol, name, accuracy, strategy_return, buy_hold_return, performance)
        VALUES (?, ?, ?, ?, ?, ?)
    ''', (
        results['symbol'],
        stock_info['name'],
        results['accuracy'],
        results['strategy_return'],
        results['buy_hold_return'],
        results['performance']
    ))
    conn.commit()

def main():
    """Fonction principale"""
    print("=== ANALYSE DE 22 ACTIONS AM√âRICAINES (S&P 500) ===")
    print("Mod√®le : LSTM (Long Short-Term Memory)")
    print(f"Univers : {len(STOCKS)} actions (2 par secteur, 11 secteurs)")
    print()
    
    # Cr√©er la base de donn√©es
    conn = create_database()
    
    # Fixer les seeds pour la reproductibilit√©
    np.random.seed(42)
    tf.random.set_seed(42)
    
    all_results = []
    
    # Analyser chaque action
    total_stocks = len(STOCKS)
    for idx, stock_info in enumerate(STOCKS, 1):
        try:
            print(f"\n[{idx}/{total_stocks}] Traitement de {stock_info['symbol']} ({stock_info['name']})")
            
            # T√©l√©charger les donn√©es
            data = download_stock_data(stock_info['symbol'])
            
            # Analyser l'action
            results = analyze_stock(data, stock_info['symbol'])
            
            if results:
                # Sauvegarder en base
                save_to_database(conn, stock_info, results)
                all_results.append(results)
                print(f"‚úì {stock_info['symbol']} termin√© avec succ√®s")
            else:
                print(f"‚ö† {stock_info['symbol']} : Analyse non effectu√©e (donn√©es insuffisantes)")
                
        except Exception as e:
            print(f"‚úó Erreur pour {stock_info['symbol']}: {e}")
    
    
    print("\n" + "="*50)
    print("R√âSULTATS")
    print("="*50)
    
    if all_results:
        df = pd.DataFrame(all_results)
        
        print(f"\n{'='*60}")
        print(f"R√âSUM√â FINAL")
        print(f"{'='*60}")
        print(f"Actions analys√©es avec succ√®s: {len(all_results)}/{total_stocks}")
        print(f"Pr√©cision moyenne: {df['accuracy'].mean():.2%}")
        print(f"Performance moyenne: {df['performance'].mean():.2%}")
        print(f"Rendement strat√©gie moyen: {df['strategy_return'].mean():.2%}")
        print(f"Rendement Buy & Hold moyen: {df['buy_hold_return'].mean():.2%}")
        print()
        
        print("üèÜ TOP 10 PAR PERFORMANCE:")
        print("-" * 60)
        df_sorted = df.sort_values('performance', ascending=False)
        for i, (_, row) in enumerate(df_sorted.head(10).iterrows(), 1):
            print(f"{i:2d}. {row['symbol']:6s} | Performance: {row['performance']:7.2%} | "
                  f"Strat√©gie: {row['strategy_return']:7.2%} | B&H: {row['buy_hold_return']:7.2%} | "
                  f"Pr√©cision: {row['accuracy']:5.2%}")
        
        if len(df_sorted) > 10:
            print(f"\n... et {len(df_sorted) - 10} autres actions")
        
        # Sauvegarder en CSV
        df.to_csv('results.csv', index=False)
        print(f"\nR√©sultats sauvegard√©s dans 'results.csv'")
    
    conn.close()
    print("\nAnalyse termin√©e ! Base de donn√©es: 'stock_analysis.db'")

if __name__ == "__main__":
    main()
